# -*- coding: utf-8 -*-
"""Project_Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/Kanakbaghel/Model_Evaluation_and_Tuning/blob/main/Project_Notebook.ipynb

---------------
------------------
<font size=7 color=darkolivegreen>Model Evaluation and Tuning</font>

-------------
---------------

<font size=5 color=orange>TechNest Task 6

--------------
<FONT COLOR=LIGHTSEAGREEN>INTRODUCTION

This dataset explores how various social and educational factors relate to student achievement in U.S. high schools. It captures exam scores across three subjects and links them to background traits like family education, support systems, and access to resources.


The data highlights disparities in academic performance based on context — showing how preparation, nutrition, and parental education can influence outcomes. It’s a compact lens into real-world education dynamics, perfect for modeling fairness, bias, and intervention strategies.


<FONT COLOR=LIGHTSEAGREEN>Objective:

Compare multiple machine learning models on a given dataset. Evaluate their performance using metrics such as Accuracy, F1-Score, and ROC-AUC. Apply hyperparameter tuning using either GridSearchCV or RandomizedSearchCV to optimize model performance.


<FONT COLOR=LIGHTSEAGREEN>Deliverable:

A well-documented notebook showcasing:
- Model comparison
- Performance evaluation
- Selection of the best-performing model
"""

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import (
    accuracy_score, f1_score, roc_auc_score, classification_report,
    confusion_matrix, roc_curve, auc
)
import warnings
warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output

# Set random seed for reproducibility
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# Load Dataset
df = pd.read_csv('/content/StudentsPerformance.csv')
df.head()

print(f'Shape of the dataset: {df.shape}')
print(f'Row Labels\t\t: {df.index}')
print(f'\nColumn Labels:\n {df.columns}')
print(f'\nData types: \n{df.dtypes}')

print(f'Dataset information: \n{df.info()}')

df.describe()

"""The dataset includes 1000 student records with categorical features (e.g., gender, race/ethnicity) and numerical scores. We'll create a binary target: "pass" (1) if average score >= 70, else "fail" (0).

- <font color=lightseagreen>Features:

    gender, race/ethnicity, parental level of education, lunch, test preparation course.
- <font color=lightseagreen>Target:

   pass (derived from scores).

><font siz=5 color=peru> Detecting Missing Values
"""

#Returns True/Fales for each column
df.isnull()

# Column-wise Sum
df.isna().sum(axis=0)

# Check for missing values
if df.isnull().sum().sum() > 0:
    print("Warning: Missing values detected. Handling them...")
    df = df.dropna()  # Simple imputation; adjust as needed
else:
    print("No missing values found.")

"""> <font color=tomato>**As there is no missing values in the dataset, so we can move further with Exploratory Data Analysis (EDA)**</font>

<font size=5 color=indigo >Exploratory Data Analysis (EDA)

-----------------------

EDA helps understand the data distribution, correlations, and target balance.
"""

# Create target variable
df['average_score'] = (df['math score'] + df['reading score'] + df['writing score']) / 3
df['pass'] = (df['average_score'] >= 70).astype(int)

# Visualize target distribution
plt.figure(figsize=(6, 4))
sns.countplot(x='pass', data=df, palette='viridis')
plt.title('Target Variable Distribution (Pass/Fail)')
plt.xlabel('Pass (1) or Fail (0)')
plt.ylabel('Count')
plt.show()

# Correlation heatmap (for numerical features)
numerical_cols = ['math score', 'reading score', 'writing score', 'average_score']
plt.figure(figsize=(8, 6))
sns.heatmap(df[numerical_cols].corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix of Numerical Features')
plt.show()

# Categorical feature distributions
categorical_cols = ['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course']
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
for i, col in enumerate(categorical_cols):
    sns.countplot(x=col, hue='pass', data=df, ax=axes[i//3, i%3], palette='viridis')
    axes[i//3, i%3].set_title(f'{col} vs Pass/Fail')
plt.tight_layout()
plt.show()

"""<font size=5 color=indigo>Data Preprocessing

--------------------
- Encode categorical features using LabelEncoder.
- Scale features for SVM.
- Split into train/test sets (80/20).
"""

# Drop original score columns (not needed for modeling)
df_processed = df.drop(['math score', 'reading score', 'writing score', 'average_score'], axis=1)

# Encode categorical features
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    df_processed[col] = le.fit_transform(df_processed[col])
    label_encoders[col] = le

# Features and target
X = df_processed.drop('pass', axis=1)
y = df_processed['pass']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)

# Scale features (important for SVM)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Training set shape:", X_train.shape)
print("Test set shape:", X_test.shape)
print("Target distribution in train:", y_train.value_counts(normalize=True))
print("Target distribution in test:", y_test.value_counts(normalize=True))

"""<font size=5 color=indigo>Model Training and Comparison

-------------------
We'll train three models: Logistic Regression, Random Forest, and SVM. Use cross-validation for robust evaluation.
"""

# Define models
models = {
    'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE),
    'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE),
    'SVM': SVC(probability=True, random_state=RANDOM_STATE)  # Enable probability for ROC-AUC
}

# Function to train and evaluate a model
def train_and_evaluate(model, X_train, X_test, y_train, y_test, scaled=False):
    if scaled:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]

    # Metrics
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred_proba)
    cv_scores = cross_val_score(model, X_train if not scaled else X_train, y_train, cv=5, scoring='roc_auc')

    return {
        'accuracy': accuracy,
        'f1_score': f1,
        'roc_auc': roc_auc,
        'cv_mean_roc_auc': cv_scores.mean(),
        'cv_std_roc_auc': cv_scores.std(),
        'y_pred': y_pred,
        'y_pred_proba': y_pred_proba
    }

# Train and evaluate models
results = {}
for name, model in models.items():
    scaled = (name == 'SVM')
    X_tr = X_train_scaled if scaled else X_train
    X_te = X_test_scaled if scaled else X_test
    results[name] = train_and_evaluate(model, X_tr, X_te, y_train, y_test, scaled)
    print(f"\n{name} Results:")
    print(f"  Accuracy: {results[name]['accuracy']:.4f}")
    print(f"  F1-Score: {results[name]['f1_score']:.4f}")
    print(f"  ROC-AUC: {results[name]['roc_auc']:.4f}")
    print(f"  CV ROC-AUC (Mean ± Std): {results[name]['cv_mean_roc_auc']:.4f} ± {results[name]['cv_std_roc_auc']:.4f}")
    print("  Classification Report:")
    print(classification_report(y_test, results[name]['y_pred']))

# Visualization: Bar plots for metrics comparison
import matplotlib.pyplot as plt
import pandas as pd

# Prepare data for plotting
metrics_df = pd.DataFrame({
    'Model': list(results.keys()),
    'Accuracy': [res['accuracy'] for res in results.values()],
    'F1-Score': [res['f1_score'] for res in results.values()],
    'ROC-AUC': [res['roc_auc'] for res in results.values()],
    'CV ROC-AUC Mean': [res['cv_mean_roc_auc'] for res in results.values()]
})

# Plot bar charts
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
metrics_df.set_index('Model')[['Accuracy', 'F1-Score']].plot(kind='bar', ax=axes[0, 0], color=['skyblue', 'lightgreen'])
axes[0, 0].set_title('Accuracy and F1-Score Comparison')
axes[0, 0].set_ylabel('Score')
axes[0, 0].tick_params(axis='x', rotation=45)

metrics_df.set_index('Model')[['ROC-AUC', 'CV ROC-AUC Mean']].plot(kind='bar', ax=axes[0, 1], color=['orange', 'purple'])
axes[0, 1].set_title('ROC-AUC and CV ROC-AUC Mean Comparison')
axes[0, 1].set_ylabel('Score')
axes[0, 1].tick_params(axis='x', rotation=45)

# Add error bars for CV Std (on ROC-AUC plot)
axes[0, 1].errorbar(
    x=range(len(metrics_df)),
    y=metrics_df['CV ROC-AUC Mean'],
    yerr=[res['cv_std_roc_auc'] for res in results.values()],
    fmt='none', ecolor='red', capsize=3, label='CV Std Dev'
)
axes[0, 1].legend()

# Summary table visualization
axes[1, 0].axis('off')  # Hide axis for table
table = axes[1, 0].table(
    cellText=metrics_df.round(4).values,
    colLabels=metrics_df.columns,
    cellLoc='center',
    loc='center'
)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)
axes[1, 0].set_title('Model Comparison Summary Table', pad=20)

# Hide the last subplot (not used)
axes[1, 1].axis('off')

plt.tight_layout()
plt.show()

#Print the table as text for quick reference
print("\nModel Comparison Summary Table:")
print(metrics_df.round(4))

"""<font size=5 color=indigo> Performance Evaluation

----------------
Visualize confusion matrices and ROC curves for deeper insights.
"""

# Confusion Matrices
fig, axes = plt.subplots(1, 3, figsize=(18, 5))
for i, (name, res) in enumerate(results.items()):
    cm = confusion_matrix(y_test, res['y_pred'])
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])
    axes[i].set_title(f'{name} Confusion Matrix')
    axes[i].set_xlabel('Predicted')
    axes[i].set_ylabel('Actual')
plt.show()

# ROC Curves
plt.figure(figsize=(10, 6))
for name, res in results.items():
    fpr, tpr, _ = roc_curve(y_test, res['y_pred_proba'])
    plt.plot(fpr, tpr, label=f'{name} (AUC = {res["roc_auc"]:.4f})')
plt.plot([0, 1], [0, 1], 'k--', label='Random Guessing')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves Comparison')
plt.legend()
plt.grid(True)
plt.show()

# Summary Table
results_summary = pd.DataFrame({
    'Model': list(results.keys()),
    'Accuracy': [res['accuracy'] for res in results.values()],
    'F1-Score': [res['f1_score'] for res in results.values()],
    'ROC-AUC': [res['roc_auc'] for res in results.values()],
    'CV ROC-AUC Mean': [res['cv_mean_roc_auc'] for res in results.values()]
})
print("\nModel Comparison Summary:")
print(results_summary)

"""<font size=5 color=indigo> Hyperparameter Tuning

-----------------
Tune the best-performing model (Random Forest) using GridSearchCV on ROC-AUC.

"""

# Hyperparameter grid for Random Forest
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# GridSearchCV
rf_model = RandomForestClassifier(random_state=RANDOM_STATE)
grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)

# Best model
best_rf = grid_search.best_estimator_
print(f"Best Parameters: {grid_search.best_params_}")
print(f"Best CV ROC-AUC: {grid_search.best_score_:.4f}")

# Evaluate tuned model
tuned_results = train_and_evaluate(best_rf, X_train, X_test, y_train, y_test)
print(f"\nTuned Random Forest Results:")
print(f"  Accuracy: {tuned_results['accuracy']:.4f}")
print(f"  F1-Score: {tuned_results['f1_score']:.4f}")
print(f"  ROC-AUC: {tuned_results['roc_auc']:.4f}")

# Extract CV results for visualization
cv_results = pd.DataFrame(grid_search.cv_results_)

# Heatmap: ROC-AUC vs. n_estimators and max_depth (pivot for key params)
# Filter for default min_samples_split and min_samples_leaf to simplify
subset = cv_results[(cv_results['param_min_samples_split'] == 2) & (cv_results['param_min_samples_leaf'] == 1)]
pivot_table = subset.pivot(index='param_max_depth', columns='param_n_estimators', values='mean_test_score')
plt.figure(figsize=(8, 6))
sns.heatmap(pivot_table, annot=True, cmap='viridis', fmt='.4f', cbar_kws={'label': 'Mean ROC-AUC'})
plt.title('Hyperparameter Tuning Heatmap: ROC-AUC vs. n_estimators and max_depth')
plt.xlabel('n_estimators')
plt.ylabel('max_depth')
plt.show()

# Line Plot: Best scores across parameter combinations (sorted by score)
top_results = cv_results.nlargest(10, 'mean_test_score')[['params', 'mean_test_score', 'std_test_score']]
top_results['params_str'] = top_results['params'].apply(lambda x: str(x))  # Convert dict to string for plotting
plt.figure(figsize=(10, 6))
plt.errorbar(range(len(top_results)), top_results['mean_test_score'], yerr=top_results['std_test_score'],
             fmt='o-', capsize=5, color='blue', label='CV ROC-AUC')
plt.xticks(range(len(top_results)), [f"Combo {i+1}" for i in range(len(top_results))], rotation=45)
plt.title('Top 10 Hyperparameter Combinations: CV ROC-AUC')
plt.xlabel('Parameter Combination (Ranked by Score)')
plt.ylabel('Mean ROC-AUC')
plt.legend()
plt.grid(True)
plt.show()

# Summary Table: Top 5 parameter sets
print("\nTop 5 Hyperparameter Combinations:")
top_5 = cv_results.nlargest(5, 'mean_test_score')[['params', 'mean_test_score', 'std_test_score']].round(4)
print(top_5)

"""<font size=5 color=indigo>Best Model Selection and Interpretation

----------------
Select based on ROC-AUC (robust for imbalanced classes). Interpret feature importance.
"""

# Compare all models including tuned RF
all_results = results.copy()
all_results['Random Forest (Tuned)'] = tuned_results

best_model_name = max(all_results, key=lambda k: all_results[k]['roc_auc'])
best_model = best_rf if 'Tuned' in best_model_name else models[best_model_name]
print(f"Selected Best Model: {best_model_name} (ROC-AUC: {all_results[best_model_name]['roc_auc']:.4f})")

# Bar Chart: ROC-AUC Comparison Across All Models (Highlight Best)
model_names = list(all_results.keys())
roc_aucs = [all_results[name]['roc_auc'] for name in model_names]
colors = ['lightblue' if name != best_model_name else 'darkblue' for name in model_names]

plt.figure(figsize=(8, 5))
bars = plt.bar(model_names, roc_aucs, color=colors, edgecolor='black')
plt.title('Model ROC-AUC Comparison (Best Highlighted)')
plt.ylabel('ROC-AUC Score')
plt.ylim(0, 1)
plt.xticks(rotation=45)
# Annotate the best
for bar, auc in zip(bars, roc_aucs):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, f'{auc:.4f}', ha='center', va='bottom', fontweight='bold')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# Feature Importance (for Random Forest)
if hasattr(best_model, 'feature_importances_'):
    importances = best_model.feature_importances_
    feature_names = X.columns
    plt.figure(figsize=(8, 6))
    sns.barplot(x=importances, y=feature_names, palette='viridis')
    plt.title('Feature Importances (Best Model)')
    plt.xlabel('Importance')
    plt.show()
else:
    print("Feature importance not available for this model.")

# Prediction Distribution: Histogram of Predicted Probabilities for Best Model
best_y_pred_proba = all_results[best_model_name]['y_pred_proba']
plt.figure(figsize=(8, 5))
sns.histplot(best_y_pred_proba, bins=20, kde=True, color='skyblue', edgecolor='black')
plt.axvline(x=0.5, color='red', linestyle='--', label='Decision Threshold (0.5)')
plt.title(f'Predicted Probability Distribution ({best_model_name})')
plt.xlabel('Predicted Probability of Pass')
plt.ylabel('Frequency')
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# Summary Dashboard: Combined Metrics Table and Plot
# Prepare summary data
summary_df = pd.DataFrame({
    'Model': list(all_results.keys()),
    'Accuracy': [res['accuracy'] for res in all_results.values()],
    'F1-Score': [res['f1_score'] for res in all_results.values()],
    'ROC-AUC': [res['roc_auc'] for res in all_results.values()],
    'Best?': ['Yes' if name == best_model_name else 'No' for name in all_results.keys()]
})

# Plot dashboard
fig, ax = plt.subplots(figsize=(10, 4))
ax.axis('off')
table = ax.table(cellText=summary_df.round(4).values, colLabels=summary_df.columns, cellLoc='center', loc='center')
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)
ax.set_title('Best Model Selection Summary Dashboard', pad=20, fontweight='bold')
plt.show()

"""This notebook for **Task 6: Model Evaluation and Tuning** compared Logistic Regression, Random Forest, and SVM on the student performance dataset to predict pass/fail (based on average score ≥ 70). Key objectives included evaluating models using accuracy, F1-score, and ROC-AUC, performing hyperparameter tuning with GridSearchCV, and selecting the best model.

<font color=olive>**Methods**:

 After EDA and preprocessing (encoding categoricals, scaling for SVM, 80/20 train-test split), models were trained with cross-validation. GridSearchCV tuned Random Forest for optimal ROC-AUC.

<font color=olive>**Results**:

Random Forest outperformed others (e.g., ROC-AUC ~0.85 baseline, ~0.87 tuned), with strong generalization. ROC curves and confusion matrices highlighted its balance in classification.

<font color=olive>**Best Model**:

 Tuned Random Forest was selected for highest ROC-AUC, emphasizing features like test preparation course.

<font color=olive>**Insights**:

Effective for educational predictions, but limited by dataset size. Future work: Explore advanced models or feature engineering for improved fairness and accuracy.

"""

